{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da70d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "%matplotlib inline\n",
    "from numpy import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from ipywidgets import interactive\n",
    "import ipywidgets as widgets\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a85aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for reading the data:\n",
    "def read_customer_data():\n",
    "    return pd.read_csv(\"customer_analysis_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a216633",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_customer_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a390cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'state',\n",
       " 'customer_lifetime_value',\n",
       " 'response',\n",
       " 'coverage',\n",
       " 'education',\n",
       " 'effective_to_date',\n",
       " 'employmentstatus',\n",
       " 'gender',\n",
       " 'income',\n",
       " 'location_code',\n",
       " 'marital_status',\n",
       " 'monthly_premium_auto',\n",
       " 'months_since_last_claim',\n",
       " 'months_since_policy_inception',\n",
       " 'number_of_open_complaints',\n",
       " 'number_of_policies',\n",
       " 'policy_type',\n",
       " 'policy',\n",
       " 'renew_offer_type',\n",
       " 'sales_channel',\n",
       " 'total_claim_amount',\n",
       " 'vehicle_class',\n",
       " 'vehicle_size',\n",
       " 'vehicle_type',\n",
       " 'month_effective_to_date']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83166cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for standardizing column names:\n",
    "def standardize(df):\n",
    "    new_columns = df.columns.str.lower().str.replace(\" \", \"_\")\n",
    "    df.columns = new_columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19944f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for dropping customer and unnamed:_0 columns\n",
    "def drop_2cols(df):\n",
    "    df.drop(columns=[\"unnamed:_0\", \"customer\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b575de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates and reset index\n",
    "def dd(df):\n",
    "    df = df.drop_duplicates().reset_index()\n",
    "    df.drop(columns=[\"index\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b316d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing null values\n",
    "def fill_null(df):\n",
    "    df[\"number_of_open_complaints\"] = df[\"number_of_open_complaints\"].fillna(value=0)\n",
    "    df[\"vehicle_type\"] = df[\"vehicle_type\"].fillna(value=\"M\")\n",
    "    mean_months = round(df[\"months_since_last_claim\"].mean())\n",
    "    df[\"months_since_last_claim\"] = df[\"months_since_last_claim\"].fillna(value=mean_months)\n",
    "    mean_income = round(df[\"income\"].mean())\n",
    "    df[\"income\"] = df[\"income\"].fillna(value=mean_income)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e8306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a months column for the effective_to_date column only\n",
    "def month_creation(df):\n",
    "    df['effective_to_date']=pd.to_datetime(df['effective_to_date'], errors='coerce')\n",
    "    df[\"month_effective_to_date\"]=list(map(lambda date:date.strftime(format=\"%B\"),df[\"effective_to_date\"]))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85acc491",
   "metadata": {},
   "source": [
    "### Running the rest of the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff31ce25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = standardize(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a35ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = month_creation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dedee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= drop_2cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e75f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fill_null(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cb2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd51063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5b6305",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export the DataFrame into a CSV and import it to Tableau for visualization\n",
    "## like this you will save it in the same folder as the notebook is saved\n",
    "\n",
    "df.to_csv(\"customer_analysis_clean.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ff4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"customer_analysis_clean.csv\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdd376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vehicle_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82462b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"vehicle_type\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d491a567",
   "metadata": {},
   "source": [
    "## Activity 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665bd129",
   "metadata": {},
   "source": [
    "### Check the data types of the columns. Get the numeric data into dataframe called numerical and categorical columns in a dataframe called categoricals. (You can use np.number and np.object to select the numerical data types and categorical data types respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708216b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0743505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all the cathegorical columns \n",
    "[col for col in df.columns if not col in df._get_numeric_data().columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a9009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a new subset -> numerical columns as the new DF\n",
    "numerical = df._get_numeric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6589332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a new subset -> cathegorical columns as the new DF\n",
    "categorical=[i for i in df.columns if df.dtypes[i]=='object']\n",
    "categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2676cafc",
   "metadata": {},
   "source": [
    "### Check for Outliers: Use the boxplot for looking at the values for income\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d493be32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.boxplot(y='income' , x='response', data=df)\n",
    "plt.ylabel('Response by Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012ba8fa",
   "metadata": {},
   "source": [
    "Findings:\n",
    "We can see that the response rate by income is quite similar but for the ones that have low incomes. There you clearly see that a lot of low income people dont response whereas only very little do respond up to the level of 20k."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f12034",
   "metadata": {},
   "source": [
    "### Now we will try to check the normality of the numerical variables visually\n",
    "\n",
    "- Use seaborn library to construct distribution plots for the numerical variables\n",
    "- Use Matplotlib to construct histograms\n",
    "- Do the distributions for different numerical variables look like a normal distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046a8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify all the numerical columns \n",
    "[col for col in df.columns if col in df._get_numeric_data().columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181f2ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical[\"customer_lifetime_value\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0bf374",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numerical.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c433c60a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8549edd5",
   "metadata": {},
   "source": [
    "Finding:\n",
    "We see that that total_claim_amount and monthly_premium_auto could have a correlation somehow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc869bf1",
   "metadata": {},
   "source": [
    "### Do some normalization on the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930b638",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use seaborn library to construct distribution plots for the numerical variables\n",
    "# -> probablity density function\n",
    "\n",
    "sns.distplot(numerical[\"monthly_premium_auto\"], hist=True)\n",
    "\n",
    "# FINDING: Looks like a LogNromal distribution and should be changed to a more normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe3fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Box-Cox Normalization method without lambda, seamingly calculating the optimal lambda\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "monthly,l=boxcox(numerical['monthly_premium_auto'])\n",
    "# the l after monthly is the optimal lambda calculated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c146861",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f56f005",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb65d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Box-Cox Normalization method with lamda=0.02\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "monthly11=boxcox(numerical['monthly_premium_auto'],lmbda=0.02)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf9a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(monthly11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b84e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical[\"customer_lifetime_value\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68afb6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power Transformer -> normalize the data between -2 and 2\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer()\n",
    "customer_lifetime_value_normalized = pt.fit_transform(numerical[\"monthly_premium_auto\"].to_numpy().reshape(-1,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a14d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(customer_lifetime_value_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dc090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile Transformer\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "qt = QuantileTransformer()\n",
    "\n",
    "monthly2=qt.fit_transform(numerical['monthly_premium_auto'].to_numpy().reshape(-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbe4c36",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.histplot(monthly2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a9dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(numerical[\"customer_lifetime_value\"], hist=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730de531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Box-Cox Normalization method without lambda, seamingly calculating the optimal lambda\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "monthly12,l=boxcox(numerical['customer_lifetime_value'])\n",
    "# the l after monthly is the optimal lambda calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a8200",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(monthly12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e805824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FITTED LINE Between two (best) correlating variables/features \n",
    "\n",
    "sns.regplot(x='customer_lifetime_value',y='total_claim_amount', data=numerical, scatter_kws={\"color\": \"olive\"}, line_kws={\"color\": \"black\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80040507",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(numerical[\"income\"], hist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84eaa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(numerical[\"total_claim_amount\"], hist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51913b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(numerical[\"total_claim_amount\"])\n",
    "pyplot.show()\n",
    "\n",
    "# histogram\n",
    "pyplot.hist(numerical[\"total_claim_amount\"])\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ce6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like a uniform distirbution\n",
    "\n",
    "sns.distplot(numerical[\"number_of_policies\"], hist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram\n",
    "plt.hist(numerical[\"total_claim_amount\"], bins=10, density=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a33cba",
   "metadata": {},
   "source": [
    "###  Normalize (numericals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8511a17",
   "metadata": {},
   "source": [
    "### MAX ABSOLUTE SCALING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data\n",
    "numerical_max_scaled = numerical.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2eefac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the normalization technique\n",
    "for column in numerical_max_scaled.columns:\n",
    "    numerical_max_scaled[column] = numerical_max_scaled[column]  / numerical_max_scaled[column].abs().max()\n",
    "    \n",
    "# view normalized data\n",
    "display(numerical_max_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f13e457",
   "metadata": {},
   "source": [
    "### Using The MIN-MAX FEATURE SCALING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79965808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data\n",
    "numerical_min_max_scaled = numerical.copy()\n",
    "  \n",
    "# apply normalization technique\n",
    "for column in numerical_min_max_scaled.columns:\n",
    "    numerical_min_max_scaled[column] = (numerical_min_max_scaled[column] - numerical_min_max_scaled[column].min()) / (numerical_min_max_scaled[column].max() - numerical_min_max_scaled[column].min())    \n",
    "  \n",
    "# view normalized data\n",
    "numerical_min_max_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ccca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(numerical_min_max_scaled[\"income\"], hist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934948d",
   "metadata": {},
   "source": [
    "### Using The Z-SCORE METHOD -> Huge Benefit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f917c093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -> HUGE BENEFIT TO TAKE MULTIPLE NUMERICAL independent VARIABLES INTO A MODEL. They are same scaled!\n",
    "\n",
    "\n",
    "# copy the data\n",
    "numerical_z_scaled = numerical.copy()\n",
    "  \n",
    "# apply normalization techniques\n",
    "for column in numerical_z_scaled.columns:\n",
    "    numerical_z_scaled[column] = (numerical_z_scaled[column] -\n",
    "                           numerical_z_scaled[column].mean()) / numerical_z_scaled[column].std()    \n",
    "  \n",
    "# view normalized data   \n",
    "display(numerical_z_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d88d31",
   "metadata": {},
   "source": [
    "### For the numerical variables, check the multicollinearity between the features. Please note that we will use the column total_claim_amount later as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb0b73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (12,10))\n",
    "sns.heatmap(numerical.corr(), annot = True, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0522f38",
   "metadata": {},
   "source": [
    "FINDING: Largest positive correlation is between total_claim_amount and monthly_premium_auto\n",
    "this can be explained by more expensive cars require higher premium payments\n",
    "and if there is a damage, these more expensive cars create higher claim amounts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5117d867",
   "metadata": {},
   "source": [
    "### For the numerical variables, check the multicollinearity between the features. Please note that we will use the column total_claim_amount later as the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966140ad",
   "metadata": {},
   "source": [
    "RESULT: As we can see in the correlation coefficient matrix above, there is no independent \n",
    "variables/features with a higher correlation than plus/minus 0.9. Hence, we dont drop any of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb94d6",
   "metadata": {},
   "source": [
    "## Transform Categorical Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c542f5",
   "metadata": {},
   "source": [
    "### Get cathegorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844cb35f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categoricals = df.select_dtypes(object)\n",
    "categoricals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47087825",
   "metadata": {},
   "source": [
    "### Conduct One Hot Encoder data fitting on policy type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f998f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals[\"policy_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982adbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc1453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder(handle_unknown=\"ignore\")\n",
    "X = [[\"Personal Auto\", 1], [\"Corporate Auto\", 2], [\"Special Auto\", 3]]\n",
    "enc.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34081aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pd.DataFrame ([[\"Personal Auto\", 1], [\"Corporate Auto\", 2], [\"Special Auto\", 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ac048",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = preprocessing.OneHotEncoder(handle_unknown=\"ignore\")\n",
    "enc.fit_transform(input_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ff7bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "### work with two categorical variables.\n",
    "enc = preprocessing.OneHotEncoder(handle_unknown=\"ignore\")\n",
    "categorical_hot_encoder = enc.fit_transform(np.array(categoricals.loc[:,[\"policy_type\",\"marital_status\"]])).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085ee1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categorical_hot_encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974e7278",
   "metadata": {},
   "source": [
    "### Put the cathegorical clumns into numerical binary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef99d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_numerical =pd.get_dummies(categoricals)\n",
    "categorical_numerical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcec60f",
   "metadata": {},
   "source": [
    "COMMENT FOR STUFF BELOW: We tried to get the above numerical results into the respective cathegorical column. Unfortunately did not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f843654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_policy_type(n_p_t):\n",
    "    categorical.replace(replace(\"Personal Auto\", [0., 1., 0., 1., 0., 0.], \"Corporate Auto\", [1., 0., 0., 0., 1., 0.], \"Special Auto\", [0., 0., 1., 0., 0., 1.]) for i in [\"policy_type\"])\n",
    "    return(n_p_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca580f68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "categoricals1 = numericalize_policy_type(categoricals)\n",
    "categoricals1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1d5fc5",
   "metadata": {},
   "source": [
    "## Activity 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149386a6",
   "metadata": {},
   "source": [
    "# Processing Data\n",
    "(Further processing...)\n",
    "\n",
    "\n",
    "- Normalize (numerical). (done)\n",
    "- One Hot/Label Encoding (categorical).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ed9a0",
   "metadata": {},
   "source": [
    "- Concat DataFrames\n",
    "\n",
    "We decided to concatinate the numerical_z_scaled df and the categorical df. We do this below and call it z_scaled_full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9b946",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_scaled_full_df = numerical_z_scaled.join(categoricals, lsuffix=\"_left\")\n",
    "z_scaled_full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d37137",
   "metadata": {},
   "source": [
    "- X-y split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7182a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100ebf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = independent variable\n",
    "#y = independent variable\n",
    "x =z_scaled_full_df[[\"monthly_premium_auto\"]] \n",
    "y = z_scaled_full_df[[\"total_claim_amount\"]] \n",
    "x.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ff8c4",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "- Train-test split.\n",
    "- Apply linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test set split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state = 100)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9aadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply linear regression\n",
    "\n",
    "lm = LinearRegression() \n",
    "model = lm.fit(x_train,y_train)\n",
    "predictions = model.predict(x_test)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5c21a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the robustness of the model (maybe R-squared)\n",
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ed2d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Isolating alpha and beta of the linear regression\n",
    "\n",
    "print(\"The slope of the regression is: {:.5f}\".format(float(model.coef_)))\n",
    "print(\"The intercept of the regression is: {:.5f}\".format(float(model.intercept_)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef80b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Isolating alpha and beta of the linear regression\n",
    "\n",
    "print(model.coef_)\n",
    "\n",
    "print(model.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## R-Squared -> SEE BELOW\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "print(\"The R2 is {:.2f}\".format(r2_score(y,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eecba8",
   "metadata": {},
   "source": [
    "### Create the model based on non-normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = independent variable\n",
    "#y = independent variable\n",
    "xx = numerical[[\"monthly_premium_auto\"]] \n",
    "yy = numerical[[\"total_claim_amount\"]] \n",
    "xx.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c82c73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train-test split.\n",
    "\n",
    "xx_train, xx_test, yy_train, yy_test = train_test_split(xx, yy, test_size=0.3, random_state = 100)\n",
    "xx_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd58ce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yy_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0553ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "yy_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2e8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply linear regression\n",
    "\n",
    "lm1 = LinearRegression()\n",
    "lm1.fit(xx_train,yy_train)\n",
    "predicted_yy = lm1.predict(xx_test)\n",
    "predicted_yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d95b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating the robustness of the model (maybe R-squared)\n",
    "lm1.score(xx_test,yy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c294a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lm1.coef_)\n",
    "print(lm1.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8f2a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e151035",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "Description:\n",
    "- MSE.\n",
    "- RMSE.\n",
    "- MAE.\n",
    "- R2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137c7cd",
   "metadata": {},
   "source": [
    "1. MSE.\n",
    "    (Mean Squared Error) represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
    "2. RMSE.\n",
    "    (Root Mean Squared Error) is the error rate by the square root of MSE.\n",
    "3. MAE.\n",
    "    (Mean absolute error) represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
    "4. R2.\n",
    "    (Coefficient of determination) represents the coefficient of how well the values fit compared to the original values. The value from 0 to 1 interpreted as percentages. The higher the value is, the better the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70252c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MSE\n",
    "mse = mean_squared_error(yy_test, predicted_yy)\n",
    "\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2befcfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RMSE\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca6c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE -> NOT SURE IF That WORK\n",
    "\n",
    "MAE = np.mean( np.abs(yy_test - predicted_yy) )\n",
    "MAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f4a842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#R2 = model.score()\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(yy_test, predicted_yy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfdf69a",
   "metadata": {},
   "source": [
    "### Long version (did not work)\n",
    "FROM the CODE ALONG Notebook LINEAR REGRESSION, we ARE USING THE SAME CODE:\n",
    "http://localhost:8888/notebooks/Desktop/BI%20or%20DnA%20BootCamps/Ironhack/Used%20for%20Exercises/Machine%20Learining%20Linear%20Regression/Notebook_Code_Along_Linear_Regression.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5580c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "\n",
    "x = x_test\n",
    "y = y_test\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c28819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the above x-test-set data frame into a numpy array\n",
    "\n",
    "x = pd.DataFrame(x)\n",
    "x.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96071e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the above y-test-set data frame into a numpy array\n",
    "\n",
    "x = pd.DataFrame(y)\n",
    "y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0659960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope\n",
    "b = (np.mean(x*y) - (np.mean(x)*np.mean(y)) ) / ( np.mean(x*x) - np.mean(x)**2)\n",
    "# Intercept\n",
    "a = np.mean(y) - np.mean(x)*b\n",
    "\n",
    "\n",
    "print(\"The slope of the regression is: {:.2f}\".format(b))\n",
    "print(\"The intercept of the regression is: {:.2f}\".format(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9005fb91",
   "metadata": {},
   "source": [
    "## Creating a Linear Model based on the template from Rafa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd74ab1f",
   "metadata": {},
   "source": [
    "### Copied from section X-Y Split in the follwing Notebook:\n",
    "https://github.com/raafat-hantoush/IH_RH_DA_FT_AUG_2021_Labs_Activities_Solutions/blob/main/Labs_Solutions/Pandas/Lab_Customer_Analysis_Case_Study.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076b48f0",
   "metadata": {},
   "source": [
    "### Reducing amount of relevant columns/features for Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e103bd2",
   "metadata": {},
   "source": [
    "1) Plotting all the cathegorical columns to see what can be dropped/bucketed (be aware, the less (categorical) columns you have, the less dummy variable columns and hence features you ll have in your model -> did not work but normally would also add significantly to the stenght of the model. Just needs experiance to have a good judgement on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1806809",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df.select_dtypes(object):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(df[c].unique(), df[c].value_counts())\n",
    "    plt.title(c)\n",
    "    plt.show();\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b21bf1",
   "metadata": {},
   "source": [
    "2) Result of the above visualization shows that you can do the following below transformation (according to Rafa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd230ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    df=df[df['income']>0]\n",
    "    df[\"total_claim_amount\"]=round(df[\"total_claim_amount\"],0)\n",
    "    df[\"customer_lifetime_value\"]=round(df[\"customer_lifetime_value\"],0)\n",
    "    df.vehicle_class[customers_df.vehicle_class.isin([\"Luxury Car\",\"Luxury SUV\"])] = \"Luxury Car\"\n",
    "    df.vehicle_class[customers_df.vehicle_class.isin([\"Four-Door Car\",\"SUV\"])] = \"Four-Door Car\"\n",
    "    df.policy[df.policy.isin([\"Personal L3\",\"Personal L2\",\"Personal L1\"]) ] = \"Personal\"\n",
    "    df.policy[df.policy.isin([\"Corporate L3\",\"Corporate L2\",\"Corporate L1\"]) ] = \"Corporate\"\n",
    "    df.policy[df.policy.isin([\"Special L3\",\"Special L2\",\"Special L1\"]) ] = \"Special\"\n",
    "    df=df.drop(columns=[\"month\",\"education\"])\n",
    "    df.number_of_policies[df.number_of_policies>2]=3\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a7b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution would be done via this code in line with pipelining at the beginning ->\n",
    "# df = transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125f5eff",
   "metadata": {},
   "source": [
    "### X-Y Splitting \n",
    "splitting the target variable \"total_claim_amount\" from the cleaned df called df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8864aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop('total_claim_amount', axis=1)\n",
    "y=df.total_claim_amount\n",
    "\n",
    "# we drop some useless columns\n",
    "X=X.drop(columns=['month_effective_to_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271417bd",
   "metadata": {},
   "source": [
    "### Get the numeric data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d228df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_features =X._get_numeric_data()\n",
    "# numericals =data.select_dtypes(np.number)\n",
    "numericals_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d61a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rounding up the customer_lifetime_value column\n",
    "numericals_features[\"customer_lifetime_value\"]=numericals_features[\"customer_lifetime_value\"].round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33fd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9cf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4308516b",
   "metadata": {},
   "source": [
    "### Normalize via StandardScaler  -> not used going foward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##def normalize(X):\n",
    "##    X_mean=X.mean(axis=0)\n",
    "##    X_std=X.std(axis=0)\n",
    "##    X_std[X_std==0]=1.0\n",
    "##    X=(X-X_mean)/X_std\n",
    "##    return X\n",
    "\n",
    "##X_num=normalize(X_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc78b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaled=StandardScaler().fit_transform(numericals_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde57bf",
   "metadata": {},
   "source": [
    "### Get Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381a5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals_features = X.select_dtypes(object)\n",
    "categoricals_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60edf33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dummies pandas\n",
    "\n",
    "categoricals_features=pd.get_dummies(categoricals_features, drop_first=True)\n",
    "\n",
    "##pd.DataFrame(OneHotEncoder(drop='first').fit_transform(categoricals_features).toarray(),\n",
    "## columns=OneHotEncoder(drop='first').fit(categoricals_features).get_feature_names(input_features=categoricals_features.columns)).head()\n",
    "\n",
    "categoricals_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425c967",
   "metadata": {},
   "source": [
    "### Concatinating the Numerical and Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features=pd.concat([numericals_features, categoricals_features], axis=1) # concat numerical and categorical transformations\n",
    "all_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c56952",
   "metadata": {},
   "source": [
    "### Train Test Split (80% train & 20% test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(all_features, y, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ca0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c40e14",
   "metadata": {},
   "source": [
    "### Apply Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba30c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model=LinearRegression()    # model\n",
    "model.fit(X_train.to_numpy(), y_train)   # model train\n",
    "y_pred=model.predict(X_test.to_numpy())   # test model y-predictor\n",
    "y_pred_train=model.predict(X_train.to_numpy()) # train y-predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80794ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "print(\"Model-Intercept: \", model.intercept_)\n",
    "\n",
    "\n",
    "print(\"Model-Coefficient: \", model.coef_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddba314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "# 1) turn the x-values into an array\n",
    "\n",
    "np.array([12, 33, 54, 60, 55, 32, 74, 39, 2, 39, 12, 34, 40, 21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8dbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "# 2) Yield the y-predictions\n",
    "\n",
    "pred = model.predict(np.array([12, 33, 54, 60, 55, 32, 74, 39, 2, 39, 12, 34, 40, 21]).reshape(-1,1))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2901ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test,y_test) # Intermezzo -> checking for the robustness of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507427f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first plot on the top left shows the y-predictor compared to the real y data in the test set\n",
    "# we see a clear diagonal relationship which is a clear sign of a robust model\n",
    "\n",
    "fig, ax = plt.subplots(2,2)\n",
    "ax[0,0].plot(y_pred, y_test, 'o')\n",
    "ax[0,0].set_xlabel(\"y_test\")\n",
    "ax[0,0].set_ylabel(\"y_pred\")\n",
    "ax[0,0].set_title(\"Test Set -Predicted vs real\")\n",
    "\n",
    "# second plot on the top right show the error values/residuals from the above comparision distributed\n",
    "# remember that the residuals have to be normally distributed as assumption Nr. 4 for OLS\n",
    "# Homoscedasticity -> i.c. TRUE\n",
    "\n",
    "ax[0,1].hist(y_test - y_pred)\n",
    "ax[0,1].set_xlabel(\"Test y-y_pred\")\n",
    "ax[0,1].set_title(\"Test Set Residual histogram\")\n",
    "\n",
    "# Third plot on the bottom left shows the trained y-predictor compared to the real y data in the train set\n",
    "\n",
    "ax[1,0].plot(y_pred_train, y_train, 'o')\n",
    "ax[1,0].set_xlabel(\"y_train\")\n",
    "ax[1,0].set_ylabel(\"y_pred_train\")\n",
    "ax[1,0].set_title(\"Train set Predicted vs real\")\n",
    "\n",
    "\n",
    "\n",
    "# Last plot on the bottom right show the error values/residuals from the above comparision distributed\n",
    "# remember that the residuals have to be normally distributed as assumption Nr. 4 for OLS\n",
    "# Homoscedasticity -> i.c. TRUE\n",
    "\n",
    "ax[1,1].hist(y_train - y_pred_train)\n",
    "ax[1,1].set_xlabel(\"Train y-y_pred\")\n",
    "ax[1,1].set_title(\"Train Residual histogram\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fe48fc",
   "metadata": {},
   "source": [
    "### Model Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a0f95",
   "metadata": {},
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e4eaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "train_mse=mse(y_train,model.predict(X_train.to_numpy()))\n",
    "\n",
    "test_mse=mse(y_test,model.predict(X_test.to_numpy()))\n",
    "\n",
    "print ('train MSE: {} -- test MSE: {}'.format(train_mse, test_mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331c73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949035b8",
   "metadata": {},
   "source": [
    "RMSE -> squaring MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06092a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('train RMSE: {} -- test RMSE: {}'.format(train_mse**.5, test_mse**.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0088542",
   "metadata": {},
   "source": [
    "MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adb32c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "\n",
    "train_mae=mae(y_train,model.predict(X_train.to_numpy()))\n",
    "test_mae=mae(y_test,model.predict(X_test.to_numpy()))\n",
    "\n",
    "print ('train MAE: {} -- test MAE: {}'.format(train_mse, test_mse))\n",
    "\n",
    "# Same result as for MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e612f8",
   "metadata": {},
   "source": [
    "R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19933d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2= model.score(X_test.to_numpy(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b29ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('train R2: {} -- test R2: {}'.format(model.score(X_train.to_numpy(), y_train),\n",
    "                                            model.score(X_test.to_numpy(), y_test)))\n",
    "\n",
    "# Same result as for model.score above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d69a5",
   "metadata": {},
   "source": [
    "adjusted R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=X_train.shape[0]\n",
    "p=1\n",
    "x = (1-r2)\n",
    "y = (N-1) / (N-p-1)\n",
    "adj_rsquared = (1 - (x * y))\n",
    "print(\"Adjusted-R2 : \" , adj_rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff60176",
   "metadata": {},
   "source": [
    "### Feature ImportanceÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe9f45b",
   "metadata": {},
   "source": [
    "Here we see which of the features contribute most to the value to predicting the target value. The higher its coeffient (in absolute terms) the more it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4384658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances = pd.DataFrame(data={\n",
    "    'Attribute': X_train.columns,\n",
    "    'Importance': model.coef_\n",
    "})\n",
    "features_importances = features_importances.sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d609c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288388fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(x=features_importances['Attribute'], height=features_importances['Importance'], color='#087E8B')\n",
    "plt.title('Feature importances obtained from coefficients', size=20)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf79928",
   "metadata": {},
   "outputs": [],
   "source": [
    "giving the features.\n",
    "\n",
    "how can we use this model now in practive?\n",
    "\n",
    "How can we visualize the model with the fitted line?\n",
    "\n",
    "We actually have a section in the jupyter notbook. There is no one dimensional fitted line, but multi-dimensional\n",
    "\n",
    " include real data to estimate values. and how do we treat the standardization?\n",
    "-> you need to put the data into the standardscaler again (same functions) then you can apply it again \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "get models in statsmodel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
